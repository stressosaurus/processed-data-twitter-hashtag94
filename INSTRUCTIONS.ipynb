{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter Hashtag 94 Data\n",
    "#### Alex John Quijano\n",
    "\n",
    "### Preamble\n",
    "\n",
    "**GITHUB.** <a href=\"https://github.com/stressosaurus/twitter-hashtag-94-data\">twitter-hashtag-94-data</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# function to get cleaned data set\n",
    "def get_cleaned_tweets_data(TYPE,LABEL,LANGUAGE,CLASS=''):\n",
    "    if CLASS == '':\n",
    "        file_t = 'clean-data/'+TYPE+'-'+LABEL+'-tweets'+CLASS+'-'+LANGUAGE+'.pkl'\n",
    "    elif CLASS == 'duplicates':\n",
    "        file_t = 'clean-data/'+TYPE+'-'+LABEL+'-tweets-'+CLASS+'-'+LANGUAGE+'.pkl'\n",
    "    elif CLASS == 'liwc':\n",
    "        file_t = 'clean-data-liwc/LIWC2015 Results ('+TYPE+'-'+LABEL+'-tweets-'+CLASS+'-'+LANGUAGE+'.csv).csv'\n",
    "        \n",
    "    try:       \n",
    "        if CLASS == '' or CLASS == 'duplicates':\n",
    "            df = pd.read_pickle(file_t)\n",
    "        elif CLASS == 'liwc':\n",
    "            df = pd.read_csv(file_t,index_col=0)\n",
    "        else:\n",
    "            print('No data ... Try again.')\n",
    "            df = None\n",
    "        return df\n",
    "    except:\n",
    "        print('ERROR: data file does not exist or input parameters are incorrect.')\n",
    "        print('Please download the necessary files first before loading the data.')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing Details\n",
    "\n",
    "**Scraper.** The Tweets from Twitter are scraped using the <a href=\"https://github.com/twintproject/twint\">Twint</a> software. When scraping, a specific hashtag is entered as a query to request relevant tweets that contain that hashtag at a particular range of time. In this case, we have 94 hashtags scraped in the time-frame January 2013 to December 2020. Below is a diagram of the text processing of tweets. Twint's purpose is to scrape tweets without the use of a Twitter API to avoid most of the limits. What it does is using the search function on twitter and scrapes the search results accordingly.\n",
    "\n",
    "**Time Frame.** The tweets was scraped in February 2021 using the time-frame January 2013 to December 2020. This means that any tweets - with a specified hashtag query - is scraped if it exists within that time-frame. Deleted tweets and users prior to February 2021 and private tweets may not exist since the scraper only takes available public tweets.\n",
    "\n",
    "**Tweet Text Processing.** The diagram below illustrates the processing pipeline where the green boxes indicate separate data savepoints. The Bag-of-Words save point is where only the processed text is saved. The original tables savepoint is where the text and other tweet information is saved (e.g. favorite counts, reply counts, etc). The LIWC tables savepoint is where the text and LIWC metrics are saved. For all savepoints, texts was lowercased and the hyperlinks are removed. The scraper can take tweets in several languages but in our case, we only take the English texts. The username tag (e.g. @username) is anonymized by replacing it by the character \"[usn]\". For all trailing usernames, it was contracted into the \"[usn]\" character. The hashtags (e.g. #hashtag) in the tweet are not fully recognized within the vocabulary of LIWC. Therefore, the hashtags are replaced with the character \"[htg]\" and trailing hashtags are contracted similar to the usernames for the LIWC tables save point only. The processed text is entered into LIWC software and the results are organized into a table where the rows are labeled with unique tweets ids and the columns contain the text and the LIWC metrics (see the <a href=\"https://repositories.lib.utexas.edu/bitstream/handle/2152/31333/LIWC2015_LanguageManual.pdf\">LIWC manual</a> for the descriptions of the LIWC metrics/columns)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"tweet-processing-pipeline-8.png\" width=\"600\" class=\"center\" onerror=this.src=\"tweet-processing-pipeline-8.png\"></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty folders for downloaded data\n",
    "try: os.mkdir('clean-data')\n",
    "except: pass\n",
    "try: os.mkdir('clean-data-bow')\n",
    "except: pass\n",
    "try: os.mkdir('clean-data-liwc')\n",
    "except: pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the Preprocessed Data\n",
    "\n",
    "There are three data folders where each folder is associated with the type of data which are Bag-of-Words, Original table, or LIWC tables. All relevant files can be downloaded from Google Drive. The following lists the links to the files. \n",
    "\n",
    "1. <a href=\"https://drive.google.com/drive/folders/1oAzyTawyvIa6eZ0cWKNF1hy4_A9K8tJJ?usp=sharing\" target=\"_blank\">Bag-Of-Words</a>. This link directs you to a Google Drive folder where it contains three subfolders: <br />\n",
    "        a. \"combined\" - contains compressed text data containing all of the tweets separated by subsets.\n",
    "        b. \"group\" - contains compressed text data grouped by hashtags and separated by subsets.\n",
    "        c. \"temporal\" - contains two subfolders:\n",
    "            i. \"combined\" - contains compressed text data separated into months and by subsets.\n",
    "            ii. \"group\" - contains compressed text data separated into months, by hashtag, and by subsets.\n",
    "\n",
    "\n",
    "2. <a href=\"https://drive.google.com/drive/folders/1yK8h_e9cdBml5TQEFYOq6umMK8Ec551J?usp=sharing\" target=\"_blank\">Original Tables</a>. This link directs you to a Google Drive folder where it contains \".pkl\" files of the original tables separated by hashtags. Each file corresponds to a hashtag and it contains tweet data where the row labels are the tweet ids. A separate file containing duplicates is also in this folder.\n",
    "\n",
    "3. <a href=\"https://drive.google.com/drive/folders/1YvPjJOGLyVmF8eH0F-bsFo28MifcTuL_?usp=sharing\" target=\"_blank\">LIWC Tables</a>. This link directs you to a Google Drive folder where it contains \".csv\" files of the LIWC tables separated by hashtags. Each file corresponds to a hashtag and it contains tweet data and the LIWC metrics. The row labels of the tables are the tweet ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Preprocessed Data\n",
    "\n",
    "**Step 1.** Download necessary files. <br />\n",
    "*Example:* If you want to view the original containing the hashtag \"climatechange\", go the Google Drive folder <a href=\"https://drive.google.com/drive/folders/1yK8h_e9cdBml5TQEFYOq6umMK8Ec551J?usp=sharing\">**Original Tables**</a> and download the \".pkl\" file labeled \"hashtags-climatechange-tweets-en.pkl\".\n",
    "    \n",
    "**Step 2.** Move the file to the designated folder. <br />\n",
    "*Example:* From step 1, move the \".pkl\" file labeled \"hashtags-climatechange-tweets-en.pkl\" to the folder labeled \"clean-data\".\n",
    "\n",
    "**Step 3.** Use the scripts below to load the downloaded \".pkl\" files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set type and language\n",
    "TYPE = 'hashtags' # tweets with hashtags only\n",
    "LANG = 'en' # There is only English language available\n",
    "\n",
    "# set hashtag\n",
    "example_hashtag = 'climatechange'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6114590, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1223032947948703744</th>\n",
       "      <td>with a new decade comes new disruption: trends...</td>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1223032800544092161</th>\n",
       "      <td>[usn] how much #renewableenergy could [usn] ha...</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2020</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                  text  \\\n",
       "1223032947948703744  with a new decade comes new disruption: trends...   \n",
       "1223032800544092161  [usn] how much #renewableenergy could [usn] ha...   \n",
       "\n",
       "                    favorite_count retweet_count reply_count  year month  \n",
       "1223032947948703744             22             6           1  2020     1  \n",
       "1223032800544092161              4             4           0  2020     1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# original table\n",
    "T = get_cleaned_tweets_data(TYPE,example_hashtag,LANG,CLASS='')\n",
    "print(T.shape)\n",
    "T[T['year'] == 2020][['text','favorite_count','retweet_count','reply_count','year','month']].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6114590, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>duplicates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1747536</th>\n",
       "      <td>[usn] these companies poison water without rem...</td>\n",
       "      <td>20040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2331572</th>\n",
       "      <td>by tackling #climatechange we’re harnessing th...</td>\n",
       "      <td>8232</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text  duplicates\n",
       "1747536  [usn] these companies poison water without rem...       20040\n",
       "2331572  by tackling #climatechange we’re harnessing th...        8232"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# duplicates table\n",
    "D = get_cleaned_tweets_data(TYPE,example_hashtag,LANG,CLASS='duplicates')\n",
    "print(D.shape)\n",
    "D.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajavq\\anaconda3\\lib\\site-packages\\numpy\\lib\\arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6114590, 94)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B</th>\n",
       "      <th>Tone</th>\n",
       "      <th>Analytic</th>\n",
       "      <th>Authentic</th>\n",
       "      <th>Clout</th>\n",
       "      <th>WC</th>\n",
       "      <th>Dic</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>296769562114936832</th>\n",
       "      <td>[usn] tells [usn] [htg] will continue. calls f...</td>\n",
       "      <td>25.77</td>\n",
       "      <td>97.53</td>\n",
       "      <td>43.37</td>\n",
       "      <td>90.87</td>\n",
       "      <td>15</td>\n",
       "      <td>46.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296769401250791424</th>\n",
       "      <td>jacoby: i raise the question for discussion: w...</td>\n",
       "      <td>25.77</td>\n",
       "      <td>99.00</td>\n",
       "      <td>61.34</td>\n",
       "      <td>67.52</td>\n",
       "      <td>22</td>\n",
       "      <td>72.73</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                    B   Tone  \\\n",
       "A                                                                              \n",
       "296769562114936832  [usn] tells [usn] [htg] will continue. calls f...  25.77   \n",
       "296769401250791424  jacoby: i raise the question for discussion: w...  25.77   \n",
       "\n",
       "                    Analytic  Authentic  Clout  WC    Dic  \n",
       "A                                                          \n",
       "296769562114936832     97.53      43.37  90.87  15  46.67  \n",
       "296769401250791424     99.00      61.34  67.52  22  72.73  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LIWC table\n",
    "L = get_cleaned_tweets_data(TYPE,example_hashtag,LANG,CLASS='liwc')\n",
    "print(L.shape)\n",
    "L[L['WC'] > 10][['B','Tone','Analytic','Authentic','Clout','WC','Dic']].head(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
